/**
 * Translation Worker
 * 
 * PENDING ìƒíƒœì˜ ë²ˆì—­ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ìƒì£¼ í”„ë¡œì„¸ìŠ¤
 * - DBì—ì„œ PENDING ì‘ì—… í´ë§
 * - Pipeline API í˜¸ì¶œ
 * - ìƒíƒœ ì—…ë°ì´íŠ¸ (RUNNING â†’ DONE/FAILED)
 */

import db, { initDb } from '../app/db.js';
import { splitIntoChunks } from './chunker.js';
import { translateWithPython, restructureParagraphsWithPython } from './translate.js';
import { runCommentBotIntl } from '../app/api/dev/run-comment-bot-intl/engine.js';
import type { LanguagePack } from '../app/api/dev/run-comment-bot-intl/types.js';
import { refundTranslationQuota } from '../lib/requireAuth.js';

// ì–¸ì–´íŒ© ë™ì  ë¡œë” (NodeNext moduleResolution í˜¸í™˜)
async function loadLangPack(lang: string): Promise<LanguagePack> {
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  let mod: any;
  switch (lang) {
    case 'ko': mod = await import('../app/api/dev/run-comment-bot-intl/lang/ko.js'); break;
    case 'ja': mod = await import('../app/api/dev/run-comment-bot-intl/lang/ja.js'); break;
    case 'zh': mod = await import('../app/api/dev/run-comment-bot-intl/lang/zh.js'); break;
    case 'es': mod = await import('../app/api/dev/run-comment-bot-intl/lang/es.js'); break;
    default: mod = await import('../app/api/dev/run-comment-bot-intl/lang/en.js'); break;
  }
  return mod.default as LanguagePack;
}

// Pipeline merged into Worker - no longer using HTTP

// ê°™ì€ ì—í”¼ì†Œë“œ ë‚´ ë™ì‹œ ë²ˆì—­ ìˆ˜ (burst ë°©ì§€ë¥¼ ìœ„í•´ 3ê°œ ì œí•œ)
const MAX_CONCURRENCY = 3;

interface TranslationJob {
  id: string;
  episode_id: string;
  language: string;
  novel_id: string;
  content: string;
  source_language: string;
  author_id: string;
}

/**
 * Fetch and claim the next pending job atomically (ë‹¨ì¼ ëª¨ë“œ)
 * Also reclaims jobs stuck in RUNNING for more than 15 minutes (dead worker recovery)
 */
async function fetchAndClaimNextJob(): Promise<TranslationJob | null> {
  const result = await db.query(`
    UPDATE episode_translations
    SET 
      status = 'RUNNING',
      updated_at = NOW()
    WHERE id = (
      SELECT et.id
      FROM episode_translations et
      JOIN episodes ep ON et.episode_id = ep.id
      JOIN novels n ON ep.novel_id = n.id
      WHERE (et.status = 'PENDING'
         OR (et.status = 'RUNNING' AND et.updated_at < NOW() - INTERVAL '15 minutes'))
        AND n.deleted_at IS NULL
      ORDER BY et.created_at ASC
      LIMIT 1
      FOR UPDATE SKIP LOCKED
    )
    RETURNING 
      id,
      episode_id,
      language,
      (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id) as novel_id,
      (SELECT content FROM episodes WHERE id = episode_translations.episode_id) as content,
      (SELECT source_language FROM novels WHERE id = (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id)) as source_language,
      (SELECT author_id FROM novels WHERE id = (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id)) as author_id
  `);

  return result.rows[0] || null;
}

/**
 * Fetch and claim multiple pending jobs for the same episode (ë³‘ë ¬ ëª¨ë“œ)
 * Uses CTE for atomic episode selection + batch claim
 * Includes dead worker recovery (15min timeout reclaim)
 * // TODO: ë©€í‹° Worker í™•ì¥ ì‹œ episode-level advisory lock ê²€í† 
 */
async function fetchAndClaimNextJobs(maxConcurrency: number): Promise<TranslationJob[]> {
  const result = await db.query(`
    WITH target AS (
      SELECT et.episode_id FROM episode_translations et
      JOIN episodes ep ON et.episode_id = ep.id
      JOIN novels n ON ep.novel_id = n.id
      WHERE (et.status = 'PENDING'
         OR (et.status = 'RUNNING' AND et.updated_at < NOW() - INTERVAL '15 minutes'))
        AND n.deleted_at IS NULL
      ORDER BY et.created_at ASC
      LIMIT 1
    )
    UPDATE episode_translations
    SET status = 'RUNNING', updated_at = NOW()
    WHERE id IN (
      SELECT id FROM episode_translations
      WHERE episode_id = (SELECT episode_id FROM target)
        AND (status = 'PENDING' OR (status = 'RUNNING' AND updated_at < NOW() - INTERVAL '15 minutes'))
      LIMIT $1
      FOR UPDATE SKIP LOCKED
    )
    RETURNING 
      id,
      episode_id,
      language,
      (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id) as novel_id,
      (SELECT content FROM episodes WHERE id = episode_translations.episode_id) as content,
      (SELECT source_language FROM novels WHERE id = (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id)) as source_language,
      (SELECT author_id FROM novels WHERE id = (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id)) as author_id
  `, [maxConcurrency]);

  return result.rows;
}

/**
 * Process a job with stagger delay to prevent synchronized API bursts
 */
async function processJobWithStagger(job: TranslationJob, index: number): Promise<void> {
  if (index > 0) {
    const delay = 50 + Math.random() * 100;
    await new Promise(r => setTimeout(r, delay));
  }
  return processJob(job);
}

/**
 * Translate a single chunk with retry logic
 */
async function translateChunk(
  chunkText: string,
  language: string,
  novelId: string,
  chunkIndex: number,
  sourceLanguage: string
): Promise<string> {
  const MAX_RETRIES = 3;
  let lastError: Error | null = null;

  for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
    try {
      // Call Python translation pipeline directly (no HTTP)
      const translatedText = await translateWithPython({
        novelTitle: novelId,
        text: chunkText,
        sourceLanguage: sourceLanguage,
        targetLanguage: language
      });

      return translatedText;

    } catch (error: any) {
      lastError = error;
      if (attempt < MAX_RETRIES - 1) {
        const backoffMs = 1000 * (attempt + 1); // Exponential backoff: 1s, 2s, 3s
        console.log(`[Worker] âš ï¸  Chunk ${chunkIndex} translation error, retrying in ${backoffMs}ms...`);
        await new Promise(resolve => setTimeout(resolve, backoffMs));
      }
    }
  }

  throw lastError || new Error('Translation failed after retries');
}

/**
 * â”€â”€ ì¡°íšŒìˆ˜ ì‹œë®¬ë ˆì´ì…˜ v3.1 â€” í–‰ë™ ê¸°ë°˜ ëª¨ë¸ â”€â”€
 * 
 * GPT 2ì°¨ ê²€ì¦ ë°˜ì˜. ê°€ìƒ ë…ìê°€ ì‹¤ì œ í–‰ë™ íŒ¨í„´ìœ¼ë¡œ ì¡°íšŒìˆ˜ ìƒì„±.
 * 
 * í•µì‹¬ êµ¬ì¡°:
 *   - ê°€ìƒ ë…ì í’€ (ë©”ëª¨ë¦¬, 100ëª… ì´ˆê¸°)
 *   - ì„¸ì…˜ ê¸°ë°˜ ì •ì£¼í–‰ (70â†’56â†’45% ì—°ì‡„)
 *   - ì‹ ê·œ/ì¬ë°©ë¬¸/êµ¬ë… ë…ì ë¶„ë¦¬
 *   - ì†Œì„¤ ë‹¨ìœ„ saturation (ceiling 10,000)
 *   - deeper long tail (90ì¼+ â†’ 0.01)
 * 
 * ì°¸ê³  ë°ì´í„°:
 *   - Royal Road/Wattpad: 1â†’2í™” 42% ì´íƒˆ, ì´í›„ íšŒì°¨ë‹¹ ~5% ê°ì†Œ
 *   - YouTube: ì—…ë¡œë“œ í›„ ì²« ë©°ì¹  í”¼í¬, ì´í›„ ê¸‰ê°
 *   - Wattpad: ì •ê¸° ì—…ë°ì´íŠ¸ ì‹œ 77% ë…ì ìœ ì§€
 * 
 * ì´ì¤‘ ì‹œìŠ¤í…œ:
 *   1) ì´ í•¨ìˆ˜ = ì‹œë®¬ë ˆì´ì…˜ ì¡°íšŒìˆ˜ (í–‰ë™ ê¸°ë°˜)
 *   2) /api/episodes/[id]/view = ì‹¤ì œ í´ë¦­ ì‹œ +1
 * 
 * í–¥í›„ Cron ë¶„ë¦¬ ê°€ëŠ¥ (í˜„ì¬ëŠ” Worker setInterval)
 */

// â”€â”€ ì„¤ì •ê°’ (ì‰½ê²Œ ì¡°ì • ê°€ëŠ¥) â”€â”€
const VIEW_CONFIG = {
  INITIAL_POOL_SIZE: 100,    // ì´ˆê¸° ê°€ìƒ ë…ì ìˆ˜
  MAX_POOL_SIZE: 300,        // í’€ ìƒí•œ
  NEW_VISITOR_RATE: 0.3,     // ë¶„ë‹¹ ì‹ ê·œ ìœ ì… ê¸°ë³¸ê°’ (Ã— ì†Œì„¤ ìˆ˜)
  VIEW_CEILING: 10_000,      // ì†Œì„¤ ë‹¨ìœ„ í¬í™” ê¸°ì¤€
  SUBSCRIBED_RETURN: 0.6,    // êµ¬ë… ë…ì ì‹ í™” ì¬ë°©ë¬¸ í™•ë¥  (60%)
};

// â”€â”€ ê°€ìƒ ë…ì íƒ€ì… â”€â”€
interface VirtualReader {
  id: string;
  novelId: string;
  lastEp: number;
  lastVisitMin: number;
  returnRate: number;        // ë¶„ë‹¹ ì¬ë°©ë¬¸ í™•ë¥  (0.002~0.006)
  bingeDepth: number;        // ìµœëŒ€ ì—°ì† ì½ê¸° (2~4)
  status: 'active' | 'subscribed' | 'dormant';
}

interface NovelInfo {
  id: string;
  maxEp: number;
  totalViews: number;
  avgViews: number;
  hoursSinceLastEp: number;
  bingeRate: number;
  commentDensity: number;  // ëŒ“ê¸€ìˆ˜/ì¡°íšŒìˆ˜ (í”¼ë“œë°± ë£¨í”„ìš©)
  episodeMap: Map<number, string>;  // epë²ˆí˜¸ â†’ episode_id
}

// â”€â”€ Worker ìˆ˜ëª… ë™ì•ˆ ìœ ì§€ë˜ëŠ” ìƒíƒœ â”€â”€
let readerPool: Map<string, VirtualReader> | null = null;
const carryBuffer: Map<string, number> = new Map();

// â”€â”€ ì‹ ì„ ë„ â€” deeper long tail (GPT ê²€ì¦) â”€â”€
function freshness(hours: number): number {
  if (hours < 6) return 2.0;       // ë°©ê¸ˆ ì˜¬ë¼ì˜´
  if (hours < 24) return 1.5;      // ë‹¹ì¼
  if (hours < 72) return 1.0;      // 3ì¼
  if (hours < 168) return 0.5;     // 7ì¼
  if (hours < 336) return 0.3;     // 14ì¼
  if (hours < 720) return 0.05;    // 30ì¼
  if (hours < 2160) return 0.01;   // 90ì¼
  return 0.003;                     // ê±°ì˜ ì •ì§€
}

// â”€â”€ ì—…ë°ì´íŠ¸ ë¶€ìŠ¤íŠ¸ â€” ìƒˆ ì—í”¼ì†Œë“œ ì˜¬ë¼ì˜¤ë©´ ìœ ì… ì¦ê°€ â”€â”€
function updateBoost(hours: number): number {
  if (hours < 6) return 1.8;
  if (hours < 24) return 1.4;
  if (hours < 48) return 1.2;
  return 1.0;
}

// â”€â”€ ì¸ê¸°ë„ íŒ©í„° â€” ì‚¬íšŒì  ì¦í­ (log ê¸°ë°˜, í­ì£¼ ë°©ì§€) â”€â”€
function popularityFactor(views: number): number {
  return 1 + Math.log10((views || 0) + 1) * 0.15;
}

// â”€â”€ ì†Œì„¤ ë‹¨ìœ„ í¬í™” â€” ceilingì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„±ì¥ ë‘”í™” â”€â”€
function saturationFactor(avgViews: number): number {
  return 1 / (1 + avgViews / VIEW_CONFIG.VIEW_CEILING);
}

// â”€â”€ ì‘í’ˆë³„ ì •ì£¼í–‰ í™•ë¥  â€” ì¸ê¸°ì‘ì¼ìˆ˜ë¡ ë†’ìŒ â”€â”€
function calcBingeRate(totalViews: number): number {
  const popBonus = Math.min(0.1, Math.log10((totalViews || 0) + 1) * 0.03);
  return Math.min(0.8, 0.6 + popBonus);  // 0.6 ~ 0.8
}

// â”€â”€ ì†Œì„¤ ì„ íƒ ê°€ì¤‘ì¹˜ (ëŒ“ê¸€ í”¼ë“œë°± ë£¨í”„ í¬í•¨) â”€â”€
function novelWeight(novel: NovelInfo): number {
  const fresh = freshness(novel.hoursSinceLastEp);
  const popular = popularityFactor(novel.totalViews);
  const sat = saturationFactor(novel.avgViews);
  const boost = updateBoost(novel.hoursSinceLastEp);
  const cmtBoost = commentDensityBoost(novel.commentDensity * novel.totalViews, novel.totalViews);
  return fresh * popular * sat * boost * cmtBoost;
}

// â”€â”€ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì†Œì„¤ ì„ íƒ â”€â”€
function weightedSelectNovel(novels: NovelInfo[]): NovelInfo {
  const weights = novels.map(n => novelWeight(n));
  const totalWeight = weights.reduce((sum, w) => sum + w, 0);
  if (totalWeight === 0) return novels[Math.floor(Math.random() * novels.length)];

  let rand = Math.random() * totalWeight;
  for (let i = 0; i < novels.length; i++) {
    rand -= weights[i];
    if (rand <= 0) return novels[i];
  }
  return novels[novels.length - 1];
}

// â”€â”€ ì¡°íšŒìˆ˜ ê°ì‡  (ì¶œë ¥ ìŠ¤ì¼€ì¼ë§) â”€â”€
// ì‹œë®¬ë ˆì´ì…˜ ë¶„í¬ëŠ” ìœ ì§€í•˜ë˜ ì ˆëŒ€ê°’ë§Œ ì¶•ì†Œ
// carryê°€ fractionalë¡œ ëˆ„ì  â†’ Math.floor ê¸°ì¡´ ë¡œì§ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬
const VIEW_DAMPENING = 0.04;  // ~1/25 ìŠ¤ì¼€ì¼ â†’ ì£¼ë‹¹ ~1,500 views

// â”€â”€ ì •ì£¼í–‰ ì„¸ì…˜ â€” í•µì‹¬ í–‰ë™ ëª¨ë¸ â”€â”€
function simulateBingeSession(
  reader: VirtualReader,
  novel: NovelInfo,
  buffer: Map<string, number>
): void {
  let currentEp = reader.lastEp;
  let continueProb = novel.bingeRate;  // ì‘í’ˆë³„ (0.6~0.8)

  for (let i = 0; i < reader.bingeDepth; i++) {
    if (currentEp > novel.maxEp) break;

    const epId = novel.episodeMap.get(currentEp);
    if (epId) {
      // ê°ì‡  + Â±20% ì§€í„° â†’ ìì—°ìŠ¤ëŸ¬ìš´ ë…¸ì´ì¦ˆ
      const weight = VIEW_DAMPENING * (0.8 + Math.random() * 0.4);
      buffer.set(epId, (buffer.get(epId) || 0) + weight);
    }

    currentEp++;
    if (Math.random() > continueProb) break;
    continueProb *= 0.8;  // 70â†’56â†’45â†’36% ê°ì‡ 
  }

  reader.lastEp = Math.max(reader.lastEp, currentEp - 1);

  if (reader.lastEp >= novel.maxEp) {
    reader.status = 'subscribed';
  }
}

// â”€â”€ ë…ì í’€ ì´ˆê¸°í™” â”€â”€
function initReaderPool(novels: NovelInfo[]): Map<string, VirtualReader> {
  const pool = new Map<string, VirtualReader>();
  for (let i = 0; i < VIEW_CONFIG.INITIAL_POOL_SIZE; i++) {
    const novel = weightedSelectNovel(novels);
    // ê¸°ì¡´ ë…ì: ì´ë¯¸ ì–´ë”˜ê°€ê¹Œì§€ ì½ì€ ìƒíƒœ
    const lastEp = 1 + Math.floor(Math.random() * novel.maxEp);
    const isCompleted = lastEp >= novel.maxEp;

    pool.set(`r_init_${i}`, {
      id: `r_init_${i}`,
      novelId: novel.id,
      lastEp: lastEp,
      lastVisitMin: Date.now(),
      returnRate: 0.002 + Math.random() * 0.004,  // 0.002~0.006/ë¶„
      bingeDepth: 2 + Math.floor(Math.random() * 3),  // 2~4
      status: isCompleted ? 'subscribed' : 'active',
    });
  }
  console.log(`[Views] ğŸ‘¥ Reader pool initialized: ${pool.size} readers across ${novels.length} novels`);
  return pool;
}

// â”€â”€ ì‹ ê·œ ìœ ì… (Poisson ê·¼ì‚¬) â”€â”€
function generateNewVisitors(
  hour: number,
  novels: NovelInfo[],
  pool: Map<string, VirtualReader>,
  buffer: Map<string, number>
): void {
  // ì‹œê°„ëŒ€ ë³€ë™ (sin ê³¡ì„  0.6~1.4)
  const timeMul = 1.0 + 0.4 * Math.sin(hour * Math.PI / 12);
  const lambda = VIEW_CONFIG.NEW_VISITOR_RATE * novels.length * timeMul;

  // Poisson ê·¼ì‚¬: ì •ìˆ˜ ë¶€ë¶„ + ì†Œìˆ˜ ë¶€ë¶„ í™•ë¥ 
  const guaranteed = Math.floor(lambda);
  const extra = Math.random() < (lambda - guaranteed) ? 1 : 0;
  const count = guaranteed + extra;

  for (let i = 0; i < count; i++) {
    const novel = weightedSelectNovel(novels);
    const reader: VirtualReader = {
      id: `r_${Date.now()}_${Math.random().toString(36).slice(2, 6)}`,
      novelId: novel.id,
      lastEp: 1,  // ì‹ ê·œëŠ” í•­ìƒ 1í™”ë¶€í„°
      lastVisitMin: Date.now(),
      returnRate: 0.002 + Math.random() * 0.004,
      bingeDepth: 2 + Math.floor(Math.random() * 3),
      status: 'active',
    };

    // ì¦‰ì‹œ ì •ì£¼í–‰ (v3 ë²„ê·¸ìˆ˜ì •: Date.now ë¹„êµ ì œê±°)
    simulateBingeSession(reader, novel, buffer);
    pool.set(reader.id, reader);
  }

  // í’€ ê´€ë¦¬
  if (pool.size > VIEW_CONFIG.MAX_POOL_SIZE) {
    prunePool(pool);
  }
}

// â”€â”€ ê¸°ì¡´ ë…ì ì¬ë°©ë¬¸ â”€â”€
function processReturningReaders(
  pool: Map<string, VirtualReader>,
  novels: Map<string, NovelInfo>,
  buffer: Map<string, number>
): void {
  for (const reader of pool.values()) {
    if (reader.status === 'dormant') continue;

    const novel = novels.get(reader.novelId);
    if (!novel) continue;

    // â”€â”€ subscribed ë…ì: ì‹ í™” ë‚˜ì™”ìœ¼ë©´ í™•ë¥ ì  ì¬ë°©ë¬¸ â”€â”€
    if (reader.status === 'subscribed') {
      if (reader.lastEp < novel.maxEp) {
        // ìƒˆ í™” ë‚˜ì™”ë‹¤! 60% í™•ë¥ ë¡œ ì¬ë°©ë¬¸ (GPT: 100%ëŠ” ë¹„í˜„ì‹¤ì )
        if (Math.random() < VIEW_CONFIG.SUBSCRIBED_RETURN) {
          reader.lastEp++;
          reader.status = 'active';
          simulateBingeSession(reader, novel, buffer);
        }
      }
      continue;
    }

    // â”€â”€ active ë…ì: í™•ë¥ ì  ì¬ë°©ë¬¸ â”€â”€
    if (reader.lastEp >= novel.maxEp) {
      reader.status = 'subscribed';
      continue;
    }

    if (Math.random() < reader.returnRate) {
      reader.lastEp++;
      simulateBingeSession(reader, novel, buffer);
    }
  }
}

// â”€â”€ í’€ ì •ë¦¬ (ì¶©ì„± ë…ì ë³´í˜¸) â”€â”€
function prunePool(pool: Map<string, VirtualReader>): void {
  // 1. dormant ë¨¼ì € ì‚­ì œ
  for (const [id, reader] of pool) {
    if (reader.status === 'dormant') pool.delete(id);
    if (pool.size <= VIEW_CONFIG.MAX_POOL_SIZE * 0.8) return;
  }
  // 2. ê°€ì¥ ì˜¤ë˜ëœ active (ì˜¤ë˜ ì•ˆ ì˜¨ = ì´íƒˆí•œ ë…ì)
  const sortedActive = [...pool.entries()]
    .filter(([, r]) => r.status === 'active')
    .sort((a, b) => a[1].lastVisitMin - b[1].lastVisitMin);
  for (const [id] of sortedActive) {
    pool.delete(id);
    if (pool.size <= VIEW_CONFIG.MAX_POOL_SIZE * 0.8) return;
  }
  // 3. ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ì˜¤ë˜ëœ subscribed (ì¶©ì„± ë…ìëŠ” ìµœí›„ê¹Œì§€ ë³´ì¡´)
  const sortedSub = [...pool.entries()]
    .filter(([, r]) => r.status === 'subscribed')
    .sort((a, b) => a[1].lastVisitMin - b[1].lastVisitMin);
  for (const [id] of sortedSub) {
    pool.delete(id);
    if (pool.size <= VIEW_CONFIG.MAX_POOL_SIZE * 0.8) return;
  }
}

// â”€â”€ ë©”ì¸: ì¡°íšŒìˆ˜ ì—…ë°ì´íŠ¸ (1ë¶„ë§ˆë‹¤ í˜¸ì¶œ) â”€â”€
async function updateViewCounts(): Promise<void> {
  // DBì—ì„œ ëª¨ë“  published ì—í”¼ì†Œë“œ ì¡°íšŒ
  const result = await db.query(`
    SELECT 
      e.id, e.novel_id, e.ep, e.views,
      COALESCE(e.scheduled_at, e.created_at) as published_at,
      (SELECT MAX(COALESCE(e2.scheduled_at, e2.created_at)) FROM episodes e2 
       WHERE e2.novel_id = e.novel_id AND e2.status = 'published') as latest_ep_at
    FROM episodes e
    WHERE e.status = 'published'
    ORDER BY e.novel_id, e.ep
  `);

  if (result.rows.length === 0) return;

  const now = new Date();
  const currentHour = now.getUTCHours();

  // â”€â”€ ì†Œì„¤ë³„ ê·¸ë£¹í•‘ â”€â”€
  const novelMap = new Map<string, NovelInfo>();
  for (const ep of result.rows) {
    if (!novelMap.has(ep.novel_id)) {
      const hoursSinceLastEp = ep.latest_ep_at
        ? (now.getTime() - new Date(ep.latest_ep_at).getTime()) / (1000 * 60 * 60)
        : 999;
      novelMap.set(ep.novel_id, {
        id: ep.novel_id,
        maxEp: 0,
        totalViews: 0,
        avgViews: 0,
        hoursSinceLastEp,
        bingeRate: 0,
        commentDensity: 0,
        episodeMap: new Map(),
      });
    }
    const novel = novelMap.get(ep.novel_id)!;
    novel.episodeMap.set(ep.ep, ep.id);
    novel.maxEp = Math.max(novel.maxEp, ep.ep);
    novel.totalViews += (ep.views || 0);
  }

  // ëŒ“ê¸€ ë°€ë„ ì¡°íšŒ (í”¼ë“œë°± ë£¨í”„ìš©)
  try {
    const cmtResult = await db.query(`
      SELECT e.novel_id, COUNT(c.id)::int AS cnt
      FROM comments c
      JOIN episodes e ON e.id = c.episode_id
      WHERE c.created_at > NOW() - INTERVAL '7 days'
      GROUP BY e.novel_id
    `);
    for (const row of cmtResult.rows) {
      const novel = novelMap.get(row.novel_id);
      if (novel && novel.totalViews > 0) {
        novel.commentDensity = (row.cnt || 0) / novel.totalViews;
      }
    }
  } catch (e) {
    // ëŒ“ê¸€ í…Œì´ë¸” ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ (density 0 ìœ ì§€)
  }

  // avg, bingeRate ê³„ì‚°
  for (const novel of novelMap.values()) {
    novel.avgViews = novel.episodeMap.size > 0
      ? novel.totalViews / novel.episodeMap.size
      : 0;
    novel.bingeRate = calcBingeRate(novel.totalViews);
  }

  const novels = [...novelMap.values()];

  // â”€â”€ ë…ì í’€ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒ) â”€â”€
  if (!readerPool) {
    readerPool = initReaderPool(novels);
  }

  // â”€â”€ 1. ì‹ ê·œ ìœ ì… â†’ 1í™”ë¶€í„° ì •ì£¼í–‰ â”€â”€
  generateNewVisitors(currentHour, novels, readerPool, carryBuffer);

  // â”€â”€ 2. ê¸°ì¡´/êµ¬ë… ë…ì ì¬ë°©ë¬¸ â”€â”€
  processReturningReaders(readerPool, novelMap, carryBuffer);

  // â”€â”€ 3. carryBuffer â†’ DB UPDATE (fractional carry) â”€â”€
  let totalAdded = 0;
  for (const [epId, carry] of carryBuffer.entries()) {
    const addViews = Math.floor(carry);
    if (addViews > 0) {
      await db.query(
        'UPDATE episodes SET views = views + $1 WHERE id = $2',
        [addViews, epId]
      );
      carryBuffer.set(epId, carry - addViews);
      totalAdded += addViews;
    }
  }

  if (totalAdded > 0) {
    const activeCount = [...readerPool.values()].filter(r => r.status === 'active').length;
    const subCount = [...readerPool.values()].filter(r => r.status === 'subscribed').length;
    console.log(`[Views] ğŸ“Š +${totalAdded} views | pool: ${readerPool.size} (active:${activeCount} sub:${subCount})`);
  }
}

/**
 * Process a translation job with chunking
 */
async function processJob(job: TranslationJob): Promise<void> {
  const { id, episode_id, language, novel_id, content, source_language } = job;

  try {
    console.log(`[Worker] ğŸ“ Processing ${language} for ${novel_id}/${episode_id}...`);

    // Skip if target language is same as source language
    if (language === source_language) {
      console.log(`[Worker] â­ï¸  Skipping ${language} (source language)`);
      await db.query(
        `UPDATE episode_translations 
         SET status = 'DONE', 
             translated_text = $1,
             updated_at = NOW() 
         WHERE id = $2`,
        [content, id]
      );
      console.log(`[Worker] âœ… ${language} marked as DONE (source language)`);
      return;
    }

    // 0. Mark as PROCESSING
    console.log(`[Worker] ğŸ”„ Updating status to PROCESSING for job ${id}...`);
    await db.query(
      `UPDATE episode_translations 
       SET status = 'PROCESSING', 
           updated_at = NOW() 
       WHERE id = $1`,
      [id]
    );
    console.log(`[Worker] âœ… Status updated to PROCESSING`);

    // 1. Split text into chunks
    const chunks = splitIntoChunks(content, 2500);
    console.log(`[Worker] ğŸ“¦ Split into ${chunks.length} chunks`);

    // 2. Translate each chunk sequentially (preserves context)
    const translatedChunks: string[] = [];
    for (const chunk of chunks) {
      console.log(`[Worker] ğŸ”„ Translating chunk ${chunk.index + 1}/${chunks.length} (${chunk.charCount} chars)...`);
      const result = await translateChunk(chunk.text, language, novel_id, chunk.index, source_language);
      translatedChunks.push(result);
    }

    // 3. Merge results (preserves original structure)
    const mergedText = translatedChunks.join('');

    // 4. Restructure paragraphs (language-specific rhythm adjustment)
    console.log(`[Worker] ğŸ“ Restructuring paragraphs for ${language}...`);
    const finalText = await restructureParagraphsWithPython(mergedText, language);
    console.log(`[Worker] âœ… Paragraph restructuring complete`);

    // 5. Save to DB (DONE status)
    await db.query(
      `UPDATE episode_translations 
       SET translated_text = $1, 
           status = 'DONE', 
           updated_at = NOW() 
       WHERE id = $2`,
      [finalText, id]
    );

    console.log(`[Worker] âœ… ${language} completed for ${novel_id}/${episode_id}`);

  } catch (error: any) {
    // error_type ë¶„ë¥˜
    const errorMsg = error.message || 'Unknown error';
    let errorType = 'SYSTEM_ERROR';
    if (errorMsg.includes('timeout') || errorMsg.includes('ETIMEDOUT')) {
      errorType = 'TIMEOUT';
    } else if (errorMsg.includes('content') || errorMsg.includes('invalid')) {
      errorType = 'INVALID_CONTENT';
    }

    // 6. Mark as FAILED + error_type
    await db.query(
      `UPDATE episode_translations 
       SET status = 'FAILED', 
           error_message = $1,
           error_type = $2,
           updated_at = NOW() 
       WHERE id = $3`,
      [errorMsg, errorType, id]
    );

    // ğŸ”„ ë©±ë“± ì¿¼í„° í™˜ë¶ˆ (quota_refunded = FALSEì¸ ê²½ìš°ì—ë§Œ)
    try {
      const refundCheck = await db.query(
        `SELECT quota_refunded FROM episode_translations WHERE id = $1`,
        [id]
      );
      if (refundCheck.rows[0] && !refundCheck.rows[0].quota_refunded) {
        await refundTranslationQuota(job.author_id);
        await db.query(
          `UPDATE episode_translations SET quota_refunded = TRUE WHERE id = $1`,
          [id]
        );
        console.log(`[Worker] ğŸ’° Quota refunded for author ${job.author_id}`);
      }
    } catch (refundErr) {
      console.error(`[Worker] âš ï¸ Refund failed:`, refundErr);
    }

    console.error(`[Worker] âŒ ${language} failed for ${novel_id}/${episode_id}: [${errorType}] ${errorMsg}`);
  }
}

// ============================================================
// ëŒ“ê¸€-ì¡°íšŒìˆ˜ ìƒê´€ê´€ê³„ v4 â€” í™•ë¥  ê¸°ë°˜ í–‰ë™ ëª¨ì‚¬
// ============================================================

// â”€â”€ Poisson ìƒ˜í”ŒëŸ¬ (Knuth ì•Œê³ ë¦¬ì¦˜) â”€â”€
function poissonSample(Î»: number): number {
  if (Î» <= 0) return 0;
  if (Î» < 30) {
    const L = Math.exp(-Î»);
    let k = 0, p = 1;
    do { k++; p *= Math.random(); } while (p > L);
    return k - 1;
  }
  // Î» â‰¥ 30: ì •ê·œ ê·¼ì‚¬
  return Math.max(0, Math.round(Î» + Math.sqrt(Î») * gaussianRandom()));
}

function gaussianRandom(): number {
  const u1 = Math.random(), u2 = Math.random();
  return Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
}

// â”€â”€ ì†Œì„¤ë³„ Quality Latent Variable Q (log-normal + drift) â”€â”€
function simpleHash(s: string): number {
  let h = 0;
  for (let i = 0; i < s.length; i++) {
    h = ((h << 5) - h + s.charCodeAt(i)) | 0;
  }
  return Math.abs(h);
}

function generateNovelQ(novelId: string): number {
  const hash = simpleHash(novelId);
  // log-normal ë¶„í¬: ëŒ€ë¶€ë¶„ mediocre, ì†Œìˆ˜ ê³ í’ˆì§ˆ, ê·¹ì†Œìˆ˜ í­ë°œ
  const u1 = ((hash % 10000) + 1) / 10001;          // (0,1) ê· ë“±
  const u2 = (((hash * 7919) % 10000) + 1) / 10001;
  const z = Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
  const base = Math.exp(-0.15 + 0.45 * z);  // Î¼=-0.15, Ïƒ=0.45 â†’ median â‰ˆ 0.86

  // ëŠë¦° drift: ì›” ë‹¨ìœ„ Â±5%
  const monthsSinceEpoch = Math.floor(Date.now() / (30 * 86400000));
  const drift = Math.sin(hash + monthsSinceEpoch * 0.3) * 0.05;

  return Math.max(0.2, Math.min(3.0, base + drift));
}

// â”€â”€ Batch-level jitter (ì›Œì»¤ ì‹¤í–‰ ë‹¨ìœ„ë¡œ ê³ ì •) â”€â”€
let batchK = 0.08;
let batchB = 0.55;
let lastBatchJitterTime = 0;

function refreshBatchJitter(): void {
  const now = Date.now();
  // 1ì‹œê°„ë§ˆë‹¤ ê°±ì‹  (ê°™ì€ batch ë‚´ì—ì„  ê³ ì •)
  if (now - lastBatchJitterTime > 3600_000) {
    batchK = 0.08 * (0.85 + Math.random() * 0.30);  // Â±15%
    batchB = 0.55 * (0.90 + Math.random() * 0.20);  // Â±10%
    lastBatchJitterTime = now;
  }
}

// â”€â”€ í•µì‹¬ ëª¨ë¸: Î» = Q Ã— k Ã— views^(1-b) Ã— D(ep) Ã— A(age) â”€â”€
function sampleCommentCount(
  views: number, epNumber: number, daysSince: number, Q: number
): number {
  if (views <= 0) return 0;

  refreshBatchJitter();

  // ê°ì‡  íŒ©í„°
  const D = 1 / (1 + 0.08 * Math.max(0, epNumber - 1));
  const A = epNumber <= 3
    ? Math.max(0.7, 1 / (1 + 0.01 * daysSince))
    : 1 / (1 + 0.15 * daysSince);

  // Î» ê³„ì‚°
  let Î» = Q * batchK * Math.pow(views, 1 - batchB) * D * A;

  // Activation threshold: ë§¤ìš° ë‚®ì€ ì¡°íšŒìˆ˜ ì–µì œ
  if (views < 15) {
    Î» *= 0.3;
  } else if (views < 30) {
    Î» *= 0.6;
  }

  // ë¹„ìœ¨ ìƒí•œ (ìµœëŒ€ 2%)
  Î» = Math.min(Î», views * 0.02);

  // Poisson ìƒ˜í”Œë§
  return poissonSample(Î»);
}

// â”€â”€ Gini Guard (ì—°ì† ê°ì‡ ) â”€â”€
function giniGuardMultiplier(novels: NovelInfo[]): number {
  if (novels.length < 3) return 1.0;
  const sorted = novels.map(n => n.totalViews).sort((a, b) => b - a);
  const total = sorted.reduce((a, b) => a + b, 0);
  if (total <= 0) return 1.0;
  const top10pct = sorted.slice(0, Math.max(1, Math.floor(sorted.length * 0.1)));
  const top10sum = top10pct.reduce((a, b) => a + b, 0);
  const concentration = top10sum / total;
  // ì—°ì† ì§€ìˆ˜ ê°ì‡ : threshold 0.5 ì´ˆê³¼ ì‹œ ë¶€ë“œëŸ½ê²Œ ì–µì œ
  if (concentration <= 0.5) return 1.0;
  return Math.exp(-3 * (concentration - 0.5));  // 0.6â†’0.74, 0.7â†’0.55, 0.8â†’0.41
}

// â”€â”€ ëŒ“ê¸€ ë°€ë„ â†’ ì¡°íšŒìˆ˜ ë¶€ìŠ¤íŠ¸ (í”¼ë“œë°± ë£¨í”„) â”€â”€
function commentDensityBoost(recentComments: number, totalViews: number): number {
  if (totalViews <= 0) return 1.0;
  const density = recentComments / totalViews;
  // log ìŠ¤ì¼€ì¼, ìµœëŒ€ +10%
  return 1 + Math.min(0.10, Math.log10(density * 100 + 1) * 0.05);
}

/**
 * Worker ë©”ì¸ ë£¨í”„
 */

// â”€â”€ ì–¸ì–´ë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ê¸€ë¡œë²Œ ì›¹ì†Œì„¤ ë…ì ë¶„í¬ + í•œêµ­ì–´ ë¶€ìŠ¤íŠ¸) â”€â”€
const LANG_BASE_WEIGHTS: Record<string, number> = {
  'ko': 0.30,   // í•œêµ­ì–´ (ìµœê³  í€„ë¦¬í‹° â†’ ë¶€ìŠ¤íŠ¸)
  'en': 0.30,   // ì˜ì–´ê¶Œ (Royal Road, Tapas ë“±)
  'ja': 0.20,   // ì¼ë³¸ì–´ê¶Œ (ì†Œì„¤ê°€ã«ãªã‚ã† ë“±)
  'zh': 0.12,   // ì¤‘êµ­ì–´ê¶Œ (Qidian ë“±)
  'es': 0.08,   // ìŠ¤í˜ì¸ì–´ê¶Œ (ì„±ì¥ì„¸ ë†’ì€ ì‹œì¥)
};

// ê°€ì¤‘ì¹˜ì— Â±30% ëœë¤ ì§€í„° ì ìš© â†’ ì—í”¼ì†Œë“œë§ˆë‹¤ ë‹¤ë¥¸ ë¹„ìœ¨
function jitteredWeights(): Record<string, number> {
  const jittered: Record<string, number> = {};
  let total = 0;
  for (const [lang, base] of Object.entries(LANG_BASE_WEIGHTS)) {
    const jitter = 0.7 + Math.random() * 0.6; // 0.7 ~ 1.3
    jittered[lang] = base * jitter;
    total += jittered[lang];
  }
  // ì •ê·œí™” (í•© = 1.0)
  for (const lang of Object.keys(jittered)) {
    jittered[lang] /= total;
  }
  return jittered;
}

// â”€â”€ ëŒ“ê¸€ ìë™ ìƒì„± (5ë¶„ ì£¼ê¸°, ì¡°íšŒìˆ˜ ê¸°ë°˜ Poisson ëª¨ë¸) â”€â”€
async function autoGenerateComments(): Promise<void> {
  const result = await db.query(`
    SELECT e.id AS episode_id, e.novel_id, e.ep, e.created_at,
           e.views,
           COALESCE(cc.cnt, 0) AS comment_count
    FROM episodes e
    JOIN novels n ON e.novel_id = n.id
    LEFT JOIN (
      SELECT episode_id, COUNT(*) AS cnt
      FROM comments GROUP BY episode_id
    ) cc ON cc.episode_id = e.id
    WHERE e.status = 'published'
    ORDER BY e.created_at ASC
    LIMIT 100
  `);

  if (result.rows.length === 0) return;

  let totalAdded = 0;

  for (const row of result.rows) {
    const { episode_id, novel_id, ep, created_at } = row;
    const publishedAt = new Date(created_at);
    const viewCount = parseInt(row.views) || 0;
    const existing = parseInt(row.comment_count) || 0;
    const epNumber = parseInt(ep) || 1;
    const daysSince = Math.floor(
      (Date.now() - publishedAt.getTime()) / 86400000
    );
    const Q = generateNovelQ(novel_id);

    // Poisson ìƒ˜í”Œë§ìœ¼ë¡œ ëª©í‘œ ëŒ“ê¸€ ìˆ˜ ê²°ì •
    const target = sampleCommentCount(viewCount, epNumber, daysSince, Q);
    const toAdd = Math.max(0, target - existing);

    if (toAdd <= 0) continue;

    // ì—í”¼ì†Œë“œë§ˆë‹¤ ë‹¤ë¥¸ ì–¸ì–´ ë¹„ìœ¨ ìƒì„±
    const weights = jitteredWeights();
    const langAllocations: { lang: string; count: number }[] = [];
    let allocated = 0;

    const langs = Object.keys(weights);
    for (let i = 0; i < langs.length; i++) {
      const lang = langs[i];
      const isLast = i === langs.length - 1;
      const count = isLast
        ? toAdd - allocated
        : Math.round(toAdd * weights[lang]);
      if (count > 0) {
        langAllocations.push({ lang, count });
        allocated += count;
      }
    }

    console.log(
      `[CommentBot] ep${ep}: v=${viewCount} Q=${Q.toFixed(2)} `
      + `Î»â†’${target} existing=${existing} adding=${toAdd} `
      + `[${langAllocations.map(a => `${a.lang}:${a.count}`).join(' ')}]`
    );

    for (const { lang, count } of langAllocations) {
      try {
        const langPack = await loadLangPack(lang);
        const botResult = await runCommentBotIntl(
          novel_id,
          langPack,
          count,
          1.0,
          true,           // useDeep
          episode_id,
          true,           // backfill
          publishedAt,
        );
        totalAdded += botResult.inserted;
      } catch (langErr) {
        console.error(`[CommentBot]   âš ï¸ ${lang} failed:`, langErr);
      }
    }

    // ì—í”¼ì†Œë“œ ê°„ 3ì´ˆ ëŒ€ê¸°
    await new Promise(r => setTimeout(r, 3000));
  }

  if (totalAdded > 0) {
    console.log(`[CommentBot] âœ… Total: ${totalAdded} comments added`);
  }
}

// â”€â”€ ì˜ˆì•½ ëŒ“ê¸€ ê³µê°œ (1ë¶„ ì£¼ê¸°) â”€â”€
async function revealScheduledComments(): Promise<void> {
  const result = await db.query(`
    UPDATE comments SET is_hidden = FALSE
    WHERE id IN (
      SELECT id FROM comments
      WHERE is_hidden = TRUE
        AND scheduled_at IS NOT NULL
        AND scheduled_at <= NOW()
      ORDER BY scheduled_at ASC
      LIMIT 5
    )
    RETURNING id
  `);

  if (result.rows.length > 0) {
    console.log(`[Reveal] ğŸ‘ ${result.rows.length} comments revealed`);
  }
}

async function main() {
  // í™˜ê²½ ë³€ìˆ˜ í™•ì¸
  if (!process.env.OPENAI_API_KEY) {
    console.error('[Worker] âŒ Missing environment variable: OPENAI_API_KEY');
    process.exit(1);
  }

  await initDb();
  console.log('[Worker] ğŸš€ Translation Worker Started');
  console.log('[Worker] ğŸ Using Python translation_core (Pipeline merged)');
  console.log(`[Worker] âš¡ Parallel mode: max ${MAX_CONCURRENCY} per episode`);
  console.log('[Worker] â° Scheduler: checking every 60s for scheduled episodes');
  console.log('[Worker] ğŸ’¬ CommentBot: checking every 5min for commentless episodes');
  console.log('[Worker] ğŸ‘ Reveal: checking every 60s for scheduled comments');
  console.log('[Worker] ğŸ‘€ Watching for PENDING jobs...\n');

  let lastScheduleCheck = 0;
  let lastViewsUpdate = 0;
  let lastCommentBot = 0;
  let lastReveal = 0;

  while (true) {
    try {
      // â”€â”€ 1. ì˜ˆì•½ ìŠ¤ì¼€ì¤„ëŸ¬ (60ì´ˆë§ˆë‹¤) â”€â”€
      if (Date.now() - lastScheduleCheck > 60_000) {
        try {
          const published = await db.query(`
            UPDATE episodes SET status = 'published'
            WHERE status = 'scheduled' AND scheduled_at <= NOW()
            RETURNING novel_id, ep
          `);
          if (published.rowCount && published.rowCount > 0) {
            for (const row of published.rows) {
              console.log(`[Scheduler] ğŸ“¢ Published: ${row.novel_id} ep${row.ep}`);
            }
          }
        } catch (schedErr) {
          console.error('[Scheduler] âš ï¸ Error:', schedErr);
        }
        lastScheduleCheck = Date.now();
      }

      // â”€â”€ 2. ì¡°íšŒìˆ˜ ìŠ¤ì¼€ì¤„ëŸ¬ (1ë¶„ë§ˆë‹¤) â”€â”€ ì—°êµ¬ ê¸°ë°˜ ìì—°ìŠ¤ëŸ¬ìš´ ì¡°íšŒìˆ˜ ì¦ê°€
      if (Date.now() - lastViewsUpdate > 60_000) {
        try {
          await updateViewCounts();
        } catch (viewErr) {
          console.error('[Views] âš ï¸ Error:', viewErr);
        }
        lastViewsUpdate = Date.now();
      }

      // â”€â”€ 3. ëŒ“ê¸€ ìë™ ìƒì„± (5ë¶„ë§ˆë‹¤) â”€â”€
      if (Date.now() - lastCommentBot > 300_000) {
        try {
          await autoGenerateComments();
        } catch (err) {
          console.error('[CommentBot] âš ï¸ Error:', err);
        }
        lastCommentBot = Date.now();
      }

      // â”€â”€ 4. ì˜ˆì•½ ëŒ“ê¸€ ê³µê°œ (1ë¶„ë§ˆë‹¤) â”€â”€
      if (Date.now() - lastReveal > 60_000) {
        try {
          await revealScheduledComments();
        } catch (err) {
          console.error('[Reveal] âš ï¸ Error:', err);
        }
        lastReveal = Date.now();
      }

      // â”€â”€ 5. ë²ˆì—­ ì‘ì—… í´ë§ â”€â”€
      const jobs = await fetchAndClaimNextJobs(MAX_CONCURRENCY);

      if (jobs.length === 0) {
        await new Promise(resolve => setTimeout(resolve, 1000));
        continue;
      }

      if (jobs.length > 1) {
        console.log(`[Worker] ğŸš€ Parallel batch: ${jobs.length} jobs (${jobs.map(j => j.language).join(', ')})`);
      }
      await Promise.allSettled(jobs.map((job, i) => processJobWithStagger(job, i)));

    } catch (error) {
      console.error('[Worker] âš ï¸ Unexpected error:', error);
      // ì—ëŸ¬ ë°œìƒ ì‹œ 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
      await new Promise(resolve => setTimeout(resolve, 5000));
    }
  }
}

// Worker ì‹œì‘
main().catch(error => {
  console.error('[Worker] ğŸ’¥ Fatal error:', error);
  process.exit(1);
});
