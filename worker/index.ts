/**
 * Translation Worker
 * 
 * PENDING ìƒíƒœì˜ ë²ˆì—­ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ìƒì£¼ í”„ë¡œì„¸ìŠ¤
 * - DBì—ì„œ PENDING ì‘ì—… í´ë§
 * - Pipeline API í˜¸ì¶œ
 * - ìƒíƒœ ì—…ë°ì´íŠ¸ (RUNNING â†’ DONE/FAILED)
 */

import db, { initDb } from '../app/db';
import { splitIntoChunks } from './chunker';
import { translateWithPython, restructureParagraphsWithPython } from './translate';

// Pipeline merged into Worker - no longer using HTTP

// â”€â”€ ë™ì‹œ ì²˜ë¦¬ ì„¤ì • â”€â”€
const PARALLEL_ENABLED = process.env.WORKER_PARALLEL_ENABLED === 'true';
const MAX_CONCURRENCY = Math.max(1, Number(process.env.WORKER_MAX_CONCURRENCY) || 3);

interface TranslationJob {
  id: string;
  episode_id: string;
  language: string;
  novel_id: string;
  content: string;
  source_language: string;
}

/**
 * Fetch and claim the next pending job atomically (ë‹¨ì¼ ëª¨ë“œ)
 * Also reclaims jobs stuck in RUNNING for more than 15 minutes (dead worker recovery)
 */
async function fetchAndClaimNextJob(): Promise<TranslationJob | null> {
  const result = await db.query(`
    UPDATE episode_translations
    SET 
      status = 'RUNNING',
      updated_at = NOW()
    WHERE id = (
      SELECT id
      FROM episode_translations
      WHERE status = 'PENDING'
         OR (status = 'RUNNING' AND updated_at < NOW() - INTERVAL '15 minutes')
      ORDER BY created_at ASC
      LIMIT 1
      FOR UPDATE SKIP LOCKED
    )
    RETURNING 
      id,
      episode_id,
      language,
      (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id) as novel_id,
      (SELECT content FROM episodes WHERE id = episode_translations.episode_id) as content,
      (SELECT source_language FROM novels WHERE id = (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id)) as source_language
  `);

  return result.rows[0] || null;
}

/**
 * Fetch and claim multiple pending jobs for the same episode (ë³‘ë ¬ ëª¨ë“œ)
 * Uses CTE for atomic episode selection + batch claim
 * Includes dead worker recovery (15min timeout reclaim)
 * // TODO: ë©€í‹° Worker í™•ì¥ ì‹œ episode-level advisory lock ê²€í† 
 */
async function fetchAndClaimNextJobs(maxConcurrency: number): Promise<TranslationJob[]> {
  const result = await db.query(`
    WITH target AS (
      SELECT episode_id FROM episode_translations
      WHERE status = 'PENDING'
         OR (status = 'RUNNING' AND updated_at < NOW() - INTERVAL '15 minutes')
      ORDER BY created_at ASC
      LIMIT 1
    )
    UPDATE episode_translations
    SET status = 'RUNNING', updated_at = NOW()
    WHERE id IN (
      SELECT id FROM episode_translations
      WHERE episode_id = (SELECT episode_id FROM target)
        AND (status = 'PENDING' OR (status = 'RUNNING' AND updated_at < NOW() - INTERVAL '15 minutes'))
      LIMIT $1
      FOR UPDATE SKIP LOCKED
    )
    RETURNING 
      id,
      episode_id,
      language,
      (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id) as novel_id,
      (SELECT content FROM episodes WHERE id = episode_translations.episode_id) as content,
      (SELECT source_language FROM novels WHERE id = (SELECT novel_id FROM episodes WHERE id = episode_translations.episode_id)) as source_language
  `, [maxConcurrency]);

  return result.rows;
}

/**
 * Process a job with stagger delay to prevent synchronized API bursts
 */
async function processJobWithStagger(job: TranslationJob, index: number): Promise<void> {
  if (index > 0) {
    const delay = 50 + Math.random() * 100;
    await new Promise(r => setTimeout(r, delay));
  }
  return processJob(job);
}

/**
 * Translate a single chunk with retry logic
 */
async function translateChunk(
  chunkText: string,
  language: string,
  novelId: string,
  chunkIndex: number,
  sourceLanguage: string
): Promise<string> {
  const MAX_RETRIES = 3;
  let lastError: Error | null = null;

  for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
    try {
      // Call Python translation pipeline directly (no HTTP)
      const translatedText = await translateWithPython({
        novelTitle: novelId,
        text: chunkText,
        sourceLanguage: sourceLanguage,
        targetLanguage: language
      });

      return translatedText;

    } catch (error: any) {
      lastError = error;
      if (attempt < MAX_RETRIES - 1) {
        const backoffMs = 1000 * (attempt + 1); // Exponential backoff: 1s, 2s, 3s
        console.log(`[Worker] âš ï¸  Chunk ${chunkIndex} translation error, retrying in ${backoffMs}ms...`);
        await new Promise(resolve => setTimeout(resolve, backoffMs));
      }
    }
  }

  throw lastError || new Error('Translation failed after retries');
}

/**
 * â”€â”€ ì—°êµ¬ ê¸°ë°˜ ì¡°íšŒìˆ˜ ì¦ê°€ ë¡œì§ â”€â”€
 * 
 * ì°¸ê³  ë°ì´í„°:
 * - Royal Road/Wattpad: 1â†’2í™” 40~50% ì´íƒˆ, ì´í›„ íšŒì°¨ë‹¹ ~5% ê°ì†Œ
 * - YouTube ì—°êµ¬: ì—…ë¡œë“œ í›„ ì²« ë©°ì¹  í”¼í¬, ì´í›„ ê¸‰ê° â†’ ì„ í˜• ì•ˆì •í™”
 * - ê¸€ë¡œë²Œ íŠ¸ë˜í”½: ì‹œê°„ëŒ€ë³„ sin íŒŒë™ (0.7~1.0)
 * - Wattpad: ì •ê¸° ì—…ë°ì´íŠ¸ ì‹œ 77% ë…ì ìœ ì§€
 */

// ì—í”¼ì†Œë“œë³„ ë‚¨ì€ ë…ì ë¹„ìœ¨ (ì´íƒˆë¥  ê³¡ì„ )
// 1â†’2í™”ì—ì„œ í° ì´íƒˆ, ì´í›„ ì™„ë§Œ
function chapterRetention(ep: number): number {
  if (ep <= 1) return 1.0;
  if (ep === 2) return 0.58;   // 42% ì´íƒˆ (Royal Road í‰ê· )
  // 2í™” ì´í›„: íšŒì°¨ë‹¹ ì•½ 5% ê°ì†Œ (95% ìœ ì§€)
  return Math.max(0.05, 0.58 * Math.pow(0.95, ep - 2));
}

// ì‹œê°„ëŒ€ ê°€ì¤‘ì¹˜ â€” sin ê³¡ì„ ìœ¼ë¡œ ì•½í•œ ê¸€ë¡œë²Œ íŒŒë„
// ë‹¤êµ­ì  í”Œë«í¼ì´ë¯€ë¡œ ê·¹ë‹¨ì  ì°¨ì´ ì—†ì´ 0.7~1.0 ë²”ìœ„
function timeWeight(hour: number): number {
  return 0.85 + 0.15 * Math.sin(hour * Math.PI / 12);
}

// ì‹ ì„ ë„ â€” ì—…ë¡œë“œ í›„ ê²½ê³¼ ì‹œê°„ì— ë”°ë¥¸ ê°ì†Œ
// YouTube ì—°êµ¬: ì²« ë©°ì¹  í”¼í¬ â†’ ê¸‰ê° â†’ ì•ˆì •í™”
function freshness(hoursAfterCreation: number): number {
  if (hoursAfterCreation < 6) return 2.0;
  if (hoursAfterCreation < 24) return 1.5;
  if (hoursAfterCreation < 72) return 1.0;     // 3ì¼
  if (hoursAfterCreation < 168) return 0.5;    // 7ì¼
  if (hoursAfterCreation < 336) return 0.3;    // 14ì¼
  return 0.15;                                  // Long tail
}

// ì—…ë°ì´íŠ¸ ë¶€ìŠ¤íŠ¸ â€” ì†Œì„¤ì— ìƒˆ ì—í”¼ì†Œë“œê°€ ì˜¬ë¼ì˜¤ë©´ ì „ì²´ ë¶€ìŠ¤íŠ¸
// Wattpad: ì •ê¸° ì—…ë°ì´íŠ¸ ì‹œ 77% ë…ì ìœ ì§€ â†’ ì‹ ê·œ ìœ ì… ë°˜ì˜
function updateBoost(hoursSinceLastUpdate: number): number {
  if (hoursSinceLastUpdate < 6) return 1.8;    // ë§‰ ì—…ë°ì´íŠ¸ë¨
  if (hoursSinceLastUpdate < 24) return 1.4;
  if (hoursSinceLastUpdate < 48) return 1.2;
  return 1.0;                                   // íš¨ê³¼ ì†Œë©¸
}

/**
 * ëª¨ë“  published ì—í”¼ì†Œë“œì˜ ì¡°íšŒìˆ˜ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¦ê°€
 * 1ë¶„ë§ˆë‹¤ Workerì—ì„œ í˜¸ì¶œ
 * 
 * ì´ì¤‘ ì‹œìŠ¤í…œ:
 *   1) ì´ í•¨ìˆ˜ = ë´‡ ì¡°íšŒìˆ˜ (ë°±ê·¸ë¼ìš´ë“œ ìì—° ì¦ê°€)
 *   2) /api/episodes/[id]/view = ì‹¤ì œ í´ë¦­ ì‹œ +1
 */
async function updateViewCounts(): Promise<void> {
  // ëª¨ë“  published ì—í”¼ì†Œë“œ ì¡°íšŒ
  // COALESCE(scheduled_at, created_at) = ì‹¤ì œ ê³µê°œ ì‹œì 
  // â†’ ì˜ˆì•½ ì—í”¼ì†Œë“œ: scheduled_at (ê³µê°œ ì˜ˆì • ì‹œê°)
  // â†’ ì¦‰ì‹œ ê³µê°œ: created_at (ì—…ë¡œë“œ ì‹œê°)
  const result = await db.query(`
    SELECT 
      e.id, e.novel_id, e.ep, e.views,
      COALESCE(e.scheduled_at, e.created_at) as published_at,
      (SELECT MAX(COALESCE(e2.scheduled_at, e2.created_at)) FROM episodes e2 
       WHERE e2.novel_id = e.novel_id AND e2.status = 'published') as latest_ep_at
    FROM episodes e
    WHERE e.status = 'published'
    ORDER BY e.novel_id, e.ep
  `);

  if (result.rows.length === 0) return;

  const now = new Date();
  const currentHour = now.getUTCHours();
  let totalAdded = 0;

  for (const ep of result.rows) {

    // â”€â”€ ì†Œì„¤ë³„ ê³ ìœ  ê°œì„± (novel_id í•´ì‹œ ê¸°ë°˜) â”€â”€
    const novelHash = hashCode(ep.novel_id);

    // base í¸ì°¨: ë©±ë²•ì¹™(Power Law) â€” ëŒ€ë¶€ë¶„ ë‚®ê³ , ì†Œìˆ˜ë§Œ ë†’ìŒ
    // í•´ì‹œë¥¼ 0~1ë¡œ ì •ê·œí™” í›„ ì œê³± â†’ ë†’ì€ ê°’ì¼ìˆ˜ë¡ í™•ë¥  ê¸‰ê°
    const hashRatio = ((novelHash >> 0) & 0xFFFF) / 0xFFFF;  // 0~1 ê· ë“±
    const skewed = Math.pow(hashRatio, 2.5);                  // ì œê³±ìœ¼ë¡œ ê¸°ìš¸ì„
    const base = Math.round(5 + skewed * 55);                 // 5~60 ë²”ìœ„

    // ì‹œê°„ ì˜¤í”„ì…‹: ì†Œì„¤ë§ˆë‹¤ sin ê³¡ì„ ì˜ ìœ„ìƒì´ ë‹¤ë¦„ (Â±6ì‹œê°„)
    const timeOffset = ((novelHash >> 8) & 0xFF) % 12;

    // jitter ë²”ìœ„: ì†Œì„¤ë§ˆë‹¤ ë³€ë™ í­ì´ ë‹¤ë¦„ (Â±20%~Â±50%)
    const jitterRange = 0.2 + (((novelHash >> 16) & 0xFF) / 255) * 0.3;

    // ê°„í—ì  quiet/burst: ì†Œì„¤ë§ˆë‹¤ ë‹¤ë¥¸ ë¦¬ë“¬
    // í˜„ì¬ ì‹œê°„ì„ novel_idë¡œ ì‹œí”„íŠ¸í•´ ì¼ì • ì£¼ê¸°ë§ˆë‹¤ ì¡°ìš©í•´ì§€ê±°ë‚˜ í™œë°œí•´ì§
    const cycleHour = (currentHour + ((novelHash >> 24) & 0xF)) % 24;
    const burstFactor = cycleHour < 4 ? 0.3 : (cycleHour > 20 ? 1.5 : 1.0);

    // published_at ê¸°ì¤€ìœ¼ë¡œ ê²½ê³¼ ì‹œê°„ ê³„ì‚° (ì˜ˆì•½ ì—í”¼ì†Œë“œë„ ê³µê°œ ì‹œì  ê¸°ì¤€)
    const hoursAfterPublish = (now.getTime() - new Date(ep.published_at).getTime()) / (1000 * 60 * 60);
    const hoursSinceLastUpdate = ep.latest_ep_at
      ? (now.getTime() - new Date(ep.latest_ep_at).getTime()) / (1000 * 60 * 60)
      : 999;

    // ê³µì‹: base Ã— ì‹œê°„ëŒ€ Ã— ì´íƒˆë¥  Ã— ì‹ ì„ ë„ Ã— ì—…ë°ì´íŠ¸ë¶€ìŠ¤íŠ¸ Ã— ë²„ìŠ¤íŠ¸íŒ©í„°
    const viewsPerHour = base
      * timeWeight(currentHour + timeOffset)
      * chapterRetention(ep.ep)
      * freshness(hoursAfterPublish)
      * updateBoost(hoursSinceLastUpdate)
      * burstFactor;

    // 1ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜ (Ã·60) + ì†Œì„¤ë³„ jitter
    const viewsPerMin = viewsPerHour / 60;
    const jitter = (1 - jitterRange) + Math.random() * (jitterRange * 2);
    let addViews = Math.round(viewsPerMin * jitter);

    // ìµœì†Œ ë³´ì¥: ì•„ë¬´ë¦¬ ë‚®ì•„ë„ ì‹œê°„ë‹¹ 1íšŒëŠ” ì˜¬ë¼ê°€ë„ë¡ (1/60 í™•ë¥ )
    if (addViews === 0 && Math.random() < 1 / 60) {
      addViews = 1;
    }

    if (addViews > 0) {
      await db.query(
        `UPDATE episodes SET views = views + $1 WHERE id = $2`,
        [addViews, ep.id]
      );
      totalAdded += addViews;
    }
  }

  if (totalAdded > 0) {
    console.log(`[Views] ğŸ“Š +${totalAdded} views across ${result.rows.length} episodes`);
  }
}

// novel_id ë¬¸ìì—´ â†’ ì•ˆì •ì ì¸ ì •ìˆ˜ í•´ì‹œ
function hashCode(str: string): number {
  let hash = 0;
  for (let i = 0; i < str.length; i++) {
    const char = str.charCodeAt(i);
    hash = ((hash << 5) - hash) + char;
    hash |= 0; // 32bit integer
  }
  return Math.abs(hash);
}

/**
 * Process a translation job with chunking
 */
async function processJob(job: TranslationJob): Promise<void> {
  const { id, episode_id, language, novel_id, content, source_language } = job;

  try {
    console.log(`[Worker] ğŸ“ Processing ${language} for ${novel_id}/${episode_id}...`);

    // Skip if target language is same as source language
    if (language === source_language) {
      console.log(`[Worker] â­ï¸  Skipping ${language} (source language)`);
      await db.query(
        `UPDATE episode_translations 
         SET status = 'DONE', 
             translated_text = $1,
             updated_at = NOW() 
         WHERE id = $2`,
        [content, id]
      );
      console.log(`[Worker] âœ… ${language} marked as DONE (source language)`);
      return;
    }

    // 0. Mark as PROCESSING
    console.log(`[Worker] ğŸ”„ Updating status to PROCESSING for job ${id}...`);
    await db.query(
      `UPDATE episode_translations 
       SET status = 'PROCESSING', 
           updated_at = NOW() 
       WHERE id = $1`,
      [id]
    );
    console.log(`[Worker] âœ… Status updated to PROCESSING`);

    // 1. Split text into chunks
    const chunks = splitIntoChunks(content, 2500);
    console.log(`[Worker] ğŸ“¦ Split into ${chunks.length} chunks`);

    // 2. Translate each chunk sequentially (preserves context)
    const translatedChunks: string[] = [];
    for (const chunk of chunks) {
      console.log(`[Worker] ğŸ”„ Translating chunk ${chunk.index + 1}/${chunks.length} (${chunk.charCount} chars)...`);
      const result = await translateChunk(chunk.text, language, novel_id, chunk.index, source_language);
      translatedChunks.push(result);
    }

    // 3. Merge results (preserves original structure)
    const mergedText = translatedChunks.join('');

    // 4. Restructure paragraphs (language-specific rhythm adjustment)
    console.log(`[Worker] ğŸ“ Restructuring paragraphs for ${language}...`);
    const finalText = await restructureParagraphsWithPython(mergedText, language);
    console.log(`[Worker] âœ… Paragraph restructuring complete`);

    // 5. Save to DB (DONE status)
    await db.query(
      `UPDATE episode_translations 
       SET translated_text = $1, 
           status = 'DONE', 
           updated_at = NOW() 
       WHERE id = $2`,
      [finalText, id]
    );

    console.log(`[Worker] âœ… ${language} completed for ${novel_id}/${episode_id}`);

  } catch (error: any) {
    // 6. Mark as FAILED
    await db.query(
      `UPDATE episode_translations 
       SET status = 'FAILED', 
           error_message = $1, 
           updated_at = NOW() 
       WHERE id = $2`,
      [error.message || 'Unknown error', id]
    );

    console.error(`[Worker] âŒ ${language} failed for ${novel_id}/${episode_id}:`, error.message);
  }
}

/**
 * Worker ë©”ì¸ ë£¨í”„
 */
async function main() {
  // í™˜ê²½ ë³€ìˆ˜ í™•ì¸
  if (!process.env.OPENAI_API_KEY) {
    console.error('[Worker] âŒ Missing environment variable: OPENAI_API_KEY');
    process.exit(1);
  }

  await initDb();
  console.log('[Worker] ğŸš€ Translation Worker Started');
  console.log('[Worker] ğŸ Using Python translation_core (Pipeline merged)');
  console.log(`[Worker] âš¡ Mode: ${PARALLEL_ENABLED ? `PARALLEL (max ${MAX_CONCURRENCY})` : 'SEQUENTIAL'}`);
  console.log('[Worker] â° Scheduler: checking every 60s for scheduled episodes');
  console.log('[Worker] ğŸ‘€ Watching for PENDING jobs...\n');

  let lastScheduleCheck = 0;
  let lastViewsUpdate = 0;

  while (true) {
    try {
      // â”€â”€ 1. ì˜ˆì•½ ìŠ¤ì¼€ì¤„ëŸ¬ (60ì´ˆë§ˆë‹¤) â”€â”€
      if (Date.now() - lastScheduleCheck > 60_000) {
        try {
          const published = await db.query(`
            UPDATE episodes SET status = 'published'
            WHERE status = 'scheduled' AND scheduled_at <= NOW()
            RETURNING novel_id, ep
          `);
          if (published.rowCount && published.rowCount > 0) {
            for (const row of published.rows) {
              console.log(`[Scheduler] ğŸ“¢ Published: ${row.novel_id} ep${row.ep}`);
            }
          }
        } catch (schedErr) {
          console.error('[Scheduler] âš ï¸ Error:', schedErr);
        }
        lastScheduleCheck = Date.now();
      }

      // â”€â”€ 2. ì¡°íšŒìˆ˜ ìŠ¤ì¼€ì¤„ëŸ¬ (1ë¶„ë§ˆë‹¤) â”€â”€ ì—°êµ¬ ê¸°ë°˜ ìì—°ìŠ¤ëŸ¬ìš´ ì¡°íšŒìˆ˜ ì¦ê°€
      if (Date.now() - lastViewsUpdate > 60_000) {
        try {
          await updateViewCounts();
        } catch (viewErr) {
          console.error('[Views] âš ï¸ Error:', viewErr);
        }
        lastViewsUpdate = Date.now();
      }

      // â”€â”€ 3. ë²ˆì—­ ì‘ì—… í´ë§ â”€â”€
      if (PARALLEL_ENABLED) {
        // ë³‘ë ¬ ëª¨ë“œ: ê°™ì€ ì—í”¼ì†Œë“œì˜ PENDING ì‘ì—…ì„ ìµœëŒ€ Nê°œì”© ë™ì‹œ ì²˜ë¦¬
        const jobs = await fetchAndClaimNextJobs(MAX_CONCURRENCY);

        if (jobs.length === 0) {
          await new Promise(resolve => setTimeout(resolve, 1000));
          continue;
        }

        console.log(`[Worker] ğŸš€ Parallel batch: ${jobs.length} jobs (${jobs.map(j => j.language).join(', ')})`);
        await Promise.allSettled(jobs.map((job, i) => processJobWithStagger(job, i)));
      } else {
        // ìˆœì°¨ ëª¨ë“œ: 1ê°œì”© ì²˜ë¦¬ (ê¸°ë³¸)
        const job = await fetchAndClaimNextJob();

        if (!job) {
          await new Promise(resolve => setTimeout(resolve, 1000));
          continue;
        }

        await processJob(job);
      }

    } catch (error) {
      console.error('[Worker] âš ï¸ Unexpected error:', error);
      // ì—ëŸ¬ ë°œìƒ ì‹œ 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
      await new Promise(resolve => setTimeout(resolve, 5000));
    }
  }
}

// Worker ì‹œì‘
main().catch(error => {
  console.error('[Worker] ğŸ’¥ Fatal error:', error);
  process.exit(1);
});
